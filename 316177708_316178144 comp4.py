# -*- coding: utf-8 -*-
"""316177708_316178144.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EH9XqwZgzltkLST7Nd_ROrHYh1UAz3F3

# Comp 4
"""

!pip install pandas numpy scikit-learn tensorflow keras transformers

!pip install --upgrade tensorflow transformers

from google.colab import drive
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras import layers
from keras.layers import LSTM, GRU, Dense, Bidirectional, Input, Concatenate
from keras.models import Model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import BertTokenizer, TFBertModel

# Commented out IPython magic to ensure Python compatibility.
drive.mount('/content/drive',force_remount=True)
# %cd '/content/drive/My Drive/Deep Learning/Comp4'

sequence_length = 10

train_data = pd.read_excel('train_ex4_data.xlsx')
val_data = pd.read_excel('val_ex4_data.xlsx')
test_data = pd.read_excel('test_ex4_data.xlsx')

# Use BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')

def get_bert_embeddings(sentences):
    input_ids = []
    attention_masks = []

    for sent in sentences:
        encoded = tokenizer.encode_plus(
            sent,
            add_special_tokens=True,
            max_length=sequence_length,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='tf'
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    input_ids = np.concatenate(input_ids, axis=0)
    attention_masks = np.concatenate(attention_masks, axis=0)

    bert_inputs = [input_ids, attention_masks]
    embeddings = bert_model.predict(bert_inputs)

    return embeddings

# Map labels to numeric values
train_data['label'] = train_data['label'].map({'M': 0, 'F': 1})
val_data['label'] = val_data['label'].map({'M': 0, 'F': 1})

# Drop source
train_data.drop("source",  axis=1, inplace=True)
val_data.drop("source",  axis=1, inplace=True)
test_data.drop("source",  axis=1, inplace=True)

# Get BERT embeddings for training data
train_sentences = train_data['action'].values
train_embeddings = get_bert_embeddings(train_sentences)
train_embeddings = train_embeddings[0]  # Extract the embeddings from the tuple
train_labels = train_data['label'].values

# Get BERT embeddings for validation data
val_sentences = val_data['action'].values
val_embeddings = get_bert_embeddings(val_sentences)
val_embeddings = val_embeddings[0]  # Extract the embeddings from the tuple
val_labels = val_data['label'].values

# Get BERT embeddings for test data
test_sentences = test_data['action'].values
test_embeddings = get_bert_embeddings(test_sentences)
test_embeddings = test_embeddings[0]  # Extract the embeddings from the tuple

X_train = train_embeddings
y_train = train_labels

X_val = val_embeddings
y_val = val_labels

X_test = test_embeddings

# Define the RNN model
model = Sequential()
model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2), input_shape=(sequence_length, X_train.shape[2])))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

# Evaluate the model on the test set
y_pred_prob = model.predict(X_test).flatten()
y_pred = (y_pred_prob > 0.5).astype(int)

# Combine predictions with test_data
predictions = pd.DataFrame({'label': y_pred})
test_data['predicted_label'] = predictions['label']

# Group predictions by ID and choose the most common label
grouped_predictions = test_data.groupby('id')['predicted_label'].apply(lambda x: x.mode()[0]).reset_index()
grouped_predictions

grouped_predictions.rename(columns={"predicted_label": "label"}, inplace=True)
grouped_predictions

from google.colab import files
grouped_predictions.to_csv('submission.csv', encoding = 'utf-8-sig', index=False)